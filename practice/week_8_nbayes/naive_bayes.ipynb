{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language analysis\n",
    "\n",
    "* Analysis of short texts and their classification to language families"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "Dataset – Pater Noster prayers in various languages.\n",
    "\n",
    "***\n",
    "\n",
    "#### ❓ Task 1\n",
    "\n",
    "  * read dataset from file *paternoster.csv* into pandas data frame *d*\n",
    "  * show dataset sample\n",
    "  * print number of columns and rows\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language classes\n",
    "\n",
    "\n",
    "* **S** – slavic languages\n",
    "* **R** – roman languages\n",
    "* **G** – german languages\n",
    "* **F** – finnish\n",
    "\n",
    "***\n",
    "\n",
    "#### ❓ Task 2\n",
    "\n",
    "  * calculate the number of languages in every class (hint: groupby or value_counts)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "\n",
    "***\n",
    "\n",
    "#### ❓ Task 3\n",
    "\n",
    "  * create column *proc* with the text from column *text* after\n",
    "    * lower case\n",
    "    * removing the diacritics\n",
    "    * replacing any punctuation with single space\n",
    "    * trimming leading and trailing spaces\n",
    "    * create numpy vectorized function *preprocess_np* with all this functionality\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ...\n",
    "\n",
    "def remove_diac(text: str):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', text)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "preprocess_np = np.vectorize(preprocess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "* trasforming the plain text into cartesian vector space\n",
    "  * dimensions: symbols – words or ngrams\n",
    "  * values: frequency of symbol in text\n",
    "\n",
    "***\n",
    "\n",
    "#### ❓ Task 4\n",
    "\n",
    "  * create object *vec* of class CountVectorizer\n",
    "    * set maximum ftrs count to 1500\n",
    "    * fit the object with texts from column *proc*\n",
    "    * print feature names\n",
    "  * create matrix X with transformed values of *proc*\n",
    "  * answer the questions:\n",
    "    * What is the most common word in English prayer?\n",
    "    * What is the most common bigram in Czech prayer?\n",
    "\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=1500, min_df=1,\n",
       "                ngram_range=(2, 2), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering of languages\n",
    "\n",
    "* Input:\n",
    "  * vectorized matrix of frequencies\n",
    "* Parameters:\n",
    "  * **affinity** – similarity measure\n",
    "    * euclidean, l1, l2, manhattan, cosine\n",
    "  * **linkage** – clustering algorithm\n",
    "    * **single** – closest members distance\n",
    "    * **complete** – furtherst members distance\n",
    "    * **average** – average distance\n",
    "    * **ward** – minimal combined variance\n",
    "     \n",
    "***\n",
    "\n",
    "#### ❓ Task 5\n",
    "\n",
    "  * read the code bellow\n",
    "  * experiment with different parameters\n",
    "    * vectorizer parameters (analyzer, ngram_range)\n",
    "    * similarity measures\n",
    "    * clustering algorithm\n",
    "  * find best possible clustering\n",
    "    * the one most separating for language classes\n",
    "\n",
    "***  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wes_rushmore=[\"#E1BD6D\", \"#EABE94\", \"#0B775E\", \"#35274A\", \"#F2300F\"]\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "cols={'S': wes_rushmore[4], 'G': wes_rushmore[2], 'R': wes_rushmore[3], 'F': wes_rushmore[0]}\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "\n",
    "    # Children of hierarchical clustering\n",
    "    children = model.children_\n",
    "\n",
    "    # Distances between each pair of children\n",
    "    # Since we don't have this information, we can use a uniform one for plotting\n",
    "    distance = np.arange(children.shape[0])\n",
    "\n",
    "    # The number of observations contained in each cluster level\n",
    "    no_of_observations = np.arange(2, children.shape[0]+2)\n",
    "\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "    linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "    \n",
    "clustering = AgglomerativeClustering(n_clusters = 4, affinity='l2', linkage='average').fit(X.toarray())\n",
    "\n",
    "plot_dendrogram(clustering, labels=lng, distance_sort='ascending')\n",
    "plt.title('Language Dendrogram');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "\n",
    "* Model: Multinomial Naive Bayes classifier\n",
    "* Classes: \n",
    "  * **S**lavic languages\n",
    "  * **G**erman languages\n",
    "* Training set: Czech and German\n",
    "* Test set: all the other languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "y = d['class'].values\n",
    "\n",
    "mnb = MultinomialNB(alpha=100)\n",
    "\n",
    "train_lng = [0,2]\n",
    "\n",
    "mnb.fit(X[train_lng],y[train_lng])\n",
    "\n",
    "pred=mnb.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification results\n",
    "\n",
    "***\n",
    "\n",
    "#### ❓ Task 6\n",
    "\n",
    "  * read and understand the code bellow\n",
    "  * evaluate the model performance on the test set\n",
    "    * mean absolute error\n",
    "    * predictions plot\n",
    "  * experiment with parameters\n",
    "    * vectorizer parameters\n",
    "    * smoothing alpha\n",
    "  * answer the questions:\n",
    "    * does model generaly work?\n",
    "    * what is the most/least slavic and most german language in the test set?\n",
    "    * what are the results on roman languages?\n",
    "    * what about finnish?\n",
    "\n",
    "***  \n",
    "\n",
    "### Prediction on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(pred, idxs, title):\n",
    "\n",
    "    fig = plt.figure(figsize=[3+len(idxs),4]);\n",
    "    x=np.arange(len(idxs))\n",
    "    plt.bar(x-0.2, pred[idxs,0], width=0.4, label = mnb.classes_[0], color=cols[mnb.classes_[0]])\n",
    "    plt.bar(x+0.2, pred[idxs,1], width=0.4, label = mnb.classes_[1], color=cols[mnb.classes_[1]])\n",
    "    plt.legend()\n",
    "    plt.xticks(ticks=x, labels=[lng[i] for i in idxs])\n",
    "    plt.title(f\"{title}, {mnb.classes_[0]} vs {mnb.classes_[1]}\")\n",
    "    plt.xlabel('Language')\n",
    "    plt.ylabel('Prediction')\n",
    "\n",
    "    \n",
    "plot_predictions(pred, idxs=train_lng, title='Training')\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "mae_tr = metrics.mean_absolute_error(y[train_lng]==mnb.classes_[0], pred[train_lng,0])\n",
    "print(f'MAE TRAIN:  {mae_tr:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strongest features\n",
    "\n",
    "* Which features (words or ngrams) most strongly support its language?\n",
    "\n",
    "***\n",
    "\n",
    "#### ❓ Task 7\n",
    "\n",
    "  * read and understand the code bellow  \n",
    "  * modify the code to answer qustions:\n",
    "    * what are the strongest slavic, german, roman bigrams?\n",
    "\n",
    "***  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lng=d[d['class'].isin(['S','G'])].index\n",
    "mnb.fit(X[train_lng],y[train_lng])\n",
    "\n",
    "df=pd.DataFrame({'ftr':vec.get_feature_names(), mnb.classes_[0]:mnb.feature_log_prob_[0], mnb.classes_[1]:mnb.feature_log_prob_[1]})\n",
    "df['dif']=df[mnb.classes_[0]]-df[mnb.classes_[1]]\n",
    "df['odds']=np.exp(df['dif'])\n",
    "df.sort_values('dif',  inplace=True)\n",
    "df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[6,10]);\n",
    "n=10\n",
    "de=pd.concat([df.head(n), df.tail(n)])\n",
    "x=np.concatenate([np.arange(0,n,dtype=int),np.arange(n+1,2*n+1,dtype=int)])\n",
    "h1=plt.barh(x[:n],de['dif'].values[0:n],\n",
    "        color=np.repeat(cols[mnb.classes_[1]],n))\n",
    "h2=plt.barh(x[n:],de['dif'].values[n:],\n",
    "        color=np.repeat(cols[mnb.classes_[0]],n))\n",
    "plt.title('Strongest features')\n",
    "plt.yticks(ticks=x, labels=de['ftr']);\n",
    "plt.legend([h1,h2], mnb.classes_[[1,0]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final task\n",
    "\n",
    "* generalize the model for three classes (S,G,R)\n",
    "    1. train the model on three representative languages\n",
    "    2. evaluate the model results\n",
    "    3. find three languages most representative for its classes\n",
    "      - lowest mean abs. error\n",
    "    \n",
    "* decide, which class is most simmilar to finish\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
